% easychair.tex,v 3.1 2011/12/30
%
% Select appropriate paper format in your document class as
% instructed by your conference organizers. Only withtimes
% and notimes can be used in proceedings created by EasyChair
%
% The available formats are 'letterpaper' and 'a4paper' with
% the former being the default if omitted as in the example
% below.
%
\documentclass[procedia]{easychair}
%\documentclass[debug]{easychair}
%\documentclass[verbose]{easychair}
%\documentclass[notimes]{easychair}
%\documentclass[withtimes]{easychair}
%\documentclass[a4paper]{easychair}
%\documentclass[letterpaper]{easychair}

% This provides the \BibTeX macro
\usepackage{doc}
\usepackage{makeidx}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{fixmath}
\usepackage{booktabs}

% In order to save space or manage large tables or figures in a
% landcape-like text, you can use the rotating and pdflscape
% packages. Uncomment the desired from the below.
%
% \usepackage{rotating}
% \usepackage{pdflscape}

% If you plan on including some algorithm specification, we recommend
% the below package. Read more details on the custom options of the
% package documentation.
%
% \usepackage{algorithm2e}

% Some of our commands for this guide.
%
\newcommand{\easychair}{\textsf{easychair}}
\newcommand{\miktex}{MiK{\TeX}}
\newcommand{\texniccenter}{{\TeX}nicCenter}
\newcommand{\makefile}{\texttt{Makefile}}
\newcommand{\latexeditor}{LEd}

\def\procediaConference{99th Conference on Topics of
  Superb Significance (COOL 2014)}

%\makeindex

%% Front Matter
%%
% Regular title as in the article class.
%
\title{Semi-Supervised Clustering Algorithms for Grouping Scientific Articles}

% \titlerunning{} has to be set to either the main title or its shorter
% version for the running heads. When processed by
% EasyChair, this command is mandatory: a document without \titlerunning
% will be rejected by EasyChair

\titlerunning{Semi-Supervised Clustering Algorithms for Grouping Scientific Articles}

% Authors are joined by \and. Their affiliations are given by \inst, which indexes into the list
% defined using \institute
%
\author{
    Diego F. Vallejo\inst{1}%\thanks{Designed and implemented the class style}
\and
    Paulina Morillo\inst{2}%\thanks{Did numerous tests and provided a lot of suggestions}
\and
    C{\`e}sar Ferri\inst{3}%\thanks{Masterminded EasyChair and created versions 3.0--3.4 of the class style}\\
}

% Institutes for affiliations are also joined by \and,
\institute{
  Universidad de las Am{\'e}ricas, Department of Mathematics, Quito, Ecuador\\
  \email{d.vallejo@udlanet.ec}
\and
   Universidad Polit{\'e}cnica Salesiana, Research Group IDEIAGEOCA, Quito, Ecuador\\
   \email{pmorillo@ups.edu.ec}\\
\and
   Universitat Polit{\`e}cnica de Val{\`e}ncia, DSIC, Val{\`e}ncia, Spain\\
   \email{cferri@dsic.upv.es}\\
 }
%  \authorrunning{} has to be set for the shorter version of the authors' names;
% otherwise a warning will be rendered in the running heads. When processed by
% EasyChair, this command is mandatory: a document without \authorrunning
% will be rejected by EasyChair

\authorrunning{Vallejo, Morillo and Ferri}

\begin{document}

\maketitle

\keywords{Clustering with constraints, Size constraint, K-Medoids, Linear programming}

\begin{abstract}
Creating sessions in scientific conferences consists in grouping papers with common topics taking into account the size restrictions imposed
by the conference schedule. Therefore, this problem can be considered as semi-supervised clustering of documents based on their content.
  This paper aims to propose modifications in traditional clustering algorithms to incorporate size constraints in each cluster. Specifically, two new algorithms are proposed to semi-supervised clustering, based on: binary integer linear programming with cannot-link constraints and a variation of the K-Medoids algorithm, respectively. 
The applicability of the proposed semi-supervised clustering methods is illustrated by addressing the problem of automatic configuration of conference schedules by clustering articles by similarity. We include experiments, applying the new techniques, over real conferences datasets: ICMLA-2014, AAAI-2013 and AAAI-2014. The results of these experiments show that the new methods are able to solve practical and real problems.
\end{abstract}

%\setcounter{tocdepth}{2}
%{\small
%\tableofcontents}

%\section{To mention}
%
%Processing in EasyChair - number of pages.
%
%Examples of how EasyChair processes papers. Caveats (replacement of EC
%class, errors).


%------------------------------------------------------------------------------
\section{Introduction}
\label{sect:introduction}

Machine learning is defined as a subfield of artificial intelligence (AI) that addresses the study and construction of models capable of learning from the data \cite{Seg:07}. Unsupervised learning is a machine learning methodology whose task is to induce a function that presents hidden structure from unlabelled data. Clustering is an example task of unsupervised learning.
Cluster analysis has the objective of dividing data objects into groups, so that objects within the same group are very similar to each other and different from objects in other groups \cite{Tan:2005}.


In many cases the data or problems, per se, have certain implicit restrictions, which traditional clustering algorithms do not take advantage of. At present, certain restrictions of size and relations of belonging of objects to the clusters have been incorporated into the clustering process, which have demonstrated that the performance of the algorithms proposed for the solution of this type of problems increases significantly \cite{Shunzhi:2010}. In clustering with size constraints, the cluster size refers to the total number of objects in each cluster \cite{Zhang:2014}.
%and class distribution is the set of cluster sizes in the final clustering solution \cite{Zhang:2014}.

Document clustering is defined as the partitioning of a documents collection into several groups according to their content \cite{Hu:2008}. Document clustering has been applied to many fields of study, such as: information retrieval, topic detection and content tracking, all of them are intrinsically related to language \cite{Douglass:92}. For the systematic treatment of language, in this paper, natural language processing (NLP) techniques are used to characterize the documents that are intended to be grouped.

A scientific article is a research paper published in specialized journals and conferences. One of the main drawbacks that arise when organizing the sessions of a conference is the large number of topics addressed by the documents presented, which are disseminated in different areas of knowledge and structures that, a priori, seem to have no relationship. In addition, the problem becomes much more complex if the times that are allocated for each session in a conference are limited, so assigning the number of papers to be exposed in a session is restricted by a specific amount. This scenario can be categorized as a problem of document clustering with size constraints.

This work addresses the problem of the automatic generation of conference schedules by using clustering techniques oriented to the grouping of documents with size constraints. When grouping scientific documents (papers), we need to take into account similarities between some features of the papers, e.g.: abstract, title, keywords, corpus, etc. We can consider these similarities by a basic weighted averaging of the individual similarities. However, in this mixing step, we loose the property of representing the documents in an Eucludian space. Many of the existing clustering methods need to represent the instances in an Euclidean space and therefore they cannot be directly applicable for this problem.
In this paper we present two new semi-supervised clustering algorithms with size constraints that are able to solve the proposed problem: CSCLP - Clustering algorithm with Size Constraints and Linear Programming, and K-MedoidsSC - K-Medoids algorithm with Size Constraints.  These algorithms can group elements taking into account size constrains of the target clusters. Additionally, we only need to have a distance or dissimilarity matrix between the elements to be clustered (i.e. they do not require the form an Euclidean space).
%. Recently, a similar approach has been described by  \cite{kvorc06}. In this case the authors also use information from reviews to build the groups, however, this information  is not always available.  

This paper is organised as follows. Section \ref{sect:previouswork} presents the previous work related to clustering algorithms with size constraints. The formalisation of the two new clustering algorithms is described in Section \ref{sect:algorithms}. Section \ref{sect:experiments} includes simulation results and experiments for the validation of the proposed algorithms: in the first instance on multivariate benchmarking datasets and subsequently with documentary datasets, which will represent conference papers in machine learning area. The holistic methodology proposed for document clustering is also presented in this section. Finally, concluding remarks and future work are presented in Section \ref{sect:conclusions}.  

%------------------------------------------------------------------------------
\section{Previous work}
\label{sect:previouswork}

A first approximation of clustering algorithms with size constraints is presented in \cite{Hopner:2008}, where the goal is to find equal sized clusters as well as clusters of different sizes, through Fuzzy C-means algorithm (K-Means variation) and Lagrange
multipliers. There are other many proposes that have focused on the modification of classical partition algorithms (such as K-Means) for the incorporation of size constraints, for instance: \cite{Rebollo:2013} and \cite{Ganganath:2014}. In \cite{Grossi:2015}  the authors propose a constraint programming formulation of some of the most famous clustering methods: K-medoids (does not use the dissimilarity matrix as input), DBSCAN and Label Propagation.

The article \cite{Shunzhi:2010} introduces an algorithm that takes as a starting point the K-Means or MPCK-Means algorithms, to transform the size constraint problem into an integer linear programming problem (ILP). Instance-level constraints are used in the form of inequalities so that this information can be incorporated into the linear programming problem. In the work \cite{Zhang:2014}, the authors introduce an algorithm called K-MeansS that allows clustering with size constraints for K-Means algorithm. The tests have shown that this procedure provides empirical evidence that combination of different kinds of partial information might improve the performance in constrained clustering.

%On the other hand, document clustering is a topic that has been extensively studied. Researchers have worked with partitional algorithms, hierarchical algorithms, methods based on graphs, etc. In \cite{Willett:1988} there is an excellent revision of the concepts, algorithms and applications related to document clustering. In \cite{Steinbach:2000}, the authors present a comparison of different techniques of document clustering, focused on hierarchical and partitional clustering algorithms, with emphasis on performance.

Traditionally, in order to calculate the similarity between objects that have numeric attributes, this objects are represented as models that configures an Euclidean space where several distances can be applied in order to estimate similarities between elements. %In addition to this vector model, there are other similarity measures such as the Dice and Jaccard coefficients, which are normalised word overlap counts \cite{Douglass:92}. 
However, sometimes the dataset has no numeric attributes or it is necessary to mix several criteria and unify them to obtain a single metric that quantifies dissimilarities, and with this, we can derive a distance or dissimilarity matrix. In these situations we cannot directly apply clustering methods based on centroids, such as K-Means, since there is not an Euclidean space defined for the elements. One way to solve this type of problem is to use algorithms that, for the clustering process use only the dissimilarity matrix as input, such as the K-Medoids algorithm. Our new approaches only uses as inputs: the initial points as instance-level constraints and the distance/dissimilarity matrix.

%An scheme of document clustering using the K-Medoids algorithm based on the highest weighed sentence is proposed in \cite{Jha:2015}. The work \cite{Moran:2014} presents a method to find the best keywords in a machine learning conference using spectral clustering, the latter is a clustering scheme that also operates with the dissimilarity matrix. This proposal is based on the opinions of experts for the validation of the results.

%Some works combines the concepts of clustering algorithms with constraints and document clustering. For instance, the article \cite{Hu:2008} inspired by the work \cite{Willett:1988}, makes an approximation by integrating instance-level constraints in a document clustering scheme. The article presents a series of comparisons with other proposed methodologies and the results are positive at performance level.

%------------------------------------------------------------------------------

\section{Clustering Algorithms with Size Constraints: CSCLP and K-MedoidsSC}
\label{sect:algorithms}
In this section we introduce the new semi-supervised algorithms that we will use in the experiments to solve the problem of clustering with size constraints.

%------------------------------------------------------------------------------

\subsection{CSCLP - Clustering Algorithm with Size Constraints and Linear Programming}
\label{sect:csclp}
In \cite{Shunzhi:2010}, the authors presented an algorithm that produces clusters that satisfy the initial restrictions by restructuring by means of ILP the starting  clusters generated  by original clustering methods (K-Means or MPCK-Means algorithm). One of the main disadvantages of this approach is the high dependence on the quality of  the clusters that result from applying the clustering algorithm. In order to mitigate this disadvantage within the formulation proposed in this work, our approach uses only the initial points as pairwise constraints (as cannot-link constraints in semi-supervised clustering terminology) for the formation of the clusters, and through the use of binary integer linear programming (BILP) determine the membership and assignment of the instances to the clusters. In this way, the original clustering problem with size constraints becomes an optimisation problem, finding the solution efficiently through a new heuristic proposal. Additionally, the method of \cite{Shunzhi:2010} is limited to problems where we can derive a Euclidean space between elements. Our proposal is more general since we use the distances/dissimilarities matrix as input parameter.

To formalise our problem, we have used the following notation: let $x_{i}=\lbrace x_{1},x_{2},...,x_{n}\rbrace$ be a given dataset of $n$ objects, where $x_{i} \in \mathbb{R}^{m}$ and $i=1,2,...,n$. The size of a cluster $c_{j}$, obtained after a clustering process, is represented by its cardinality $|c_{j}|$. So to start the clustering process the user must specify the desire number of clusters $k$, for the generation of the initial points, and the size of the desired clusters $E_{j}=\lbrace e_{1},e_{2},...,e_{k}\rbrace$ where $j=1,2,...,k$. As mentioned above the $k$ initial points $u_{j}=\lbrace u_{1},u_{2},...,u_{k}\rbrace$ are also cannot-link constraints, which means that none of them can belong to the same cluster.

Studies related to clustering have demonstrated that the K-Means algorithm, as well as many of its variations, obtain better results if a selection of initial points method is used, different to the random choice \cite{Bradley:1998}. For this reason, the initial points for our clustering algorithms are chosen using two methods: Farthest Neighbour Technique \cite{Gonzalez:1985} and Buckshot algorithm \cite{Douglass:92}. The first one selects the $k$ farthest points from the whole dataset, and this way  the K-Medoids (or K-Means) convergence to the global optimum is insured. Buckshot is a hybrid technique whose main idea is to choose a small random sample of points (of size $\sqrt{kn}$), and then to apply a hierarchical clustering method to find $k$ clusters. The centroids of this clusters are the $k$ initial points. Once these initial $k$ points are generated, an assignment of the remaining objects to one of the clusters must be found, so it is necessary to calculate the distances/dissimilarities matrix between all the objects in the dataset.
%We use these distances to compute the cost between each starting point and the other objects, without taking into account the remaining $k-1$ seed points.

After the initial $k$ points are determined, we need to fill the clusters by means of defining an optimisation problem. For that reason, we need to formalise properly as an ILP problem the clusters to form. Consider a matrix model, where $A_{nxk}$ is a boolean matrix, whose elements depict the belonging of the documents to a particular group. Each row of the array represent an object $x_{i}$ ($i=1,2,...,n$; $n=number$ $of$ $objects$) and each column is a cluster $c_{j}$ ($j=1,2,...,k$; $k=number$ $of$ $clusters$). If an element of this Boolean matrix $a_{ij}$ takes a value equal to $1$, it implies that the object $i$ belongs to cluster $j$, in a complementary way, $a_{ij}$ indicates that object $i$ does not belong to cluster $j$.
$$A=
\bordermatrix{
            & c_1     & c_2    & \cdots & c_k   \cr
    x_1     & 1       & 0      & 0      & 0     \cr
    x_2     & 0       & 0      & 0      & 1     \cr
    \vdots  & 0       & 1      & 0      & 0     \cr
    \vdots  & 0       & 0      & 1      & 0     \cr
    x_n     & 0       & 0      & 0      & 1     \cr
            }_{nxk}
$$ 
The sum of elements of each row shows that an object can only belong to a single group: $\sum_{j=1}^k a_{ij}=1 ; \forall i=1,2,...,n $, this sum depicts the first constraints we are going to consider and we have called belonging constraints. The sum of elements of each column shows the size of each cluster: $\sum_{i=1}^n a_{ij}=e_{j} ; \forall j=1,2,...,k $, it is the second constraints we will consider and it depicts size constraints.

Following this reasoning and given the matrix of dissimilarity, we pose the problem as a linear programming problem, the objective function ($O.F.$) is given by the expression: 
$$O.F. = min (\sum_{j=1}^k \sum_{i=1}^n d_{ij}p_{ij})$$
where $d_{ij}$ is the distance of the object $i$ to the initial object $j$ and $p_{i,j}$ is an array of belonging of each object $i$ to the cluster $j$.

%------------------------------------------------------------------------------

\subsection{K-MedoidsSC - K-Medoids algorithm with Size Constraints}
\label{sect:kmedoidssc}

One disadvantage and limitation of K-MeansS algorithm \cite{Zhang:2014}, is that during the iterative refinement process, the centroids are calculated and updated in a geometric space where the coordinates of the new centroids can be calculated, therefore this algorithm can not work with problems where instances not define a Euclidean space, i.e. problems where we only have a dissimilarity or distance matrix between elements in order to know their resemblance. Inspired by the work of Zhang et al.\cite{Zhang:2014}, we propose a change to this formulation by using the K-Medoids algorithm \cite{Kaufman:1987} in the clustering process and for restrict the sizes of the clusters. 

The new formulation, is based on the minimisation of a cost function $J_{KMS}$. This function is the sum of another four cost functions. The first one $J_{KM}$ takes into account the minimisation of the distance between objects labelled to be in a cluster and an object designated as the medoid of that cluster (K-Medoids criterion \cite{Kaufman:1987}). The other three are cost functions that penalise the size of the clusters: when the desired cluster size is not achieved $J_{A}$, when cluster size is smaller than expected $J_{S}$ and when the cluster size is larger than expected $J_{L}$. The function that results is given by the expression: $$J_{KMS}=J_{KM}+\alpha J_{A}+\beta J_{S}++\gamma J_{L}$$

where $\alpha$, $\beta$, $\gamma$ are the corresponding non-negative scale parameters which represent different weights for the different penalty functions for the cluster sizes. In our case, we considered $\alpha=\beta=\gamma=1$. 

$J_{A}$ function aligns the desired cluster sizes with the obtained cluster sizes by the K-Medoids algorithm, we use the Jensen-Shannon divergence \cite{jensen} to define it. $J_{S}$ is defined as the sum of the distances between the cluster medoids $u_{j}$, which cluster sizes are smaller than expected, and the objects $x_{i}$ that belongs to another cluster but are closer from their medoids: $$J_{S}=\sum_{j=1}^m d(x_i,u_j) $$ 

where $p$ is the number of clusters with cluster size smaller than expected. On the other hand, $J_{L}$ is defined as the sum of the distances between the cluster medoids, which cluster sizes are larger than expected, and the objects that belongs to the same cluster but are farther from their medoids: $$J_{L}=\sum_{i=1}^p d(x_i,u_j)$$ 

where $m$ is the number of clusters with cluster size larger than expected. We modified the algorithm, so instead of calculating a new centroid for each iteration, the new algorithm chooses the object (medoid) that minimises $J_{KMS}$ function. 

Since the aforementioned penalties are applied directly to the distance or dissimilarity matrix (whose values are normalized between 0 and 1), a correction factor $c_{f}$ must be applied to prevent the matrix from being annulled or negative values are obtained when applying the penalties. This correction factor has a domain between $1 < c_{f}\leqslant \infty$ and multiplies all the elements of the matrix by a value imposed by the user. K-MedoidsSC algorithm performs an iterative process until it converges. If the number of imposed iterations has been met, and when any of the clusters $c_{j}$ does not meet the desired cluster size $E_{j}$, the algorithm returns to the original distance matrix to reassign the remaining or missing objects only over the clusters where their size is different to the desired group size.

If we compare K-MedoidsSC with respect to CSCLP we find important differences. CSCLP solves the semi-supervised clustering problem as an optimisation problem, while K-MedoidsSC starts from the clusters formed by K-Medoids algorithm and then the clusters are re-arranged trying to satisfy the size constrains. %However, if the starting clusters are very diverse from the goal cluster sizes, in some cases K-MedoidsSC is not able to find a valid grouping (we will see this in the experimental section). 
In the other hand, given the optimisation approach of CSCLP, probably for problems with a high number of elements to order this algorithm will not be very efficient. In this case of problems, K-MedoidsSC would be a better option.

%------------------------------------------------------------------------------

\section{Experiments}
\label{sect:experiments}
In this section we include some experiments in order to assess the performance of the proposed methods. We conduct two different settings of experiments. First, we test the validity of the methods over small and well-known datasets. Secondly, we analyse the performance over a document clustering scenario, i.e, we employ real datasets that contain data about papers of scientific conferences.

%------------------------------------------------------------------------------

\subsection{Validation of CSCLP and K-MedoidsSC Algorithms}
\label{sect:validation}

Prior to the experimentation on documentary datasets, it is necessary to perform tests to evaluate the effectiveness and performance of the two new heuristic algorithms proposed. Three well-known classification datasets (Iris, Wine and Seeds) have been used from the UCI Machine Learning Repository (University of California Irvine). The datasets characteristics and their full descriptions can be found in \cite{UCI:1998}. All the datasets have three classes, and we use the class label as cluster identifier (the class distributions represent size constrains).

Table \ref{tb:tab1} shows a comparison of the different cluster sizes, if we apply to the datasets, algorithms without size constraints such as: Agglomerative Hierarchical Clustering - Farthest Point Algorithm (AHC-FPA) and K-Medoids together with the two new proposals with size constraints (CSCLP and K-MedoidsSC), in contrast to the real value of clusters size (determined through dataset-specific data). In the case of  K-Medoids, CSCLP and K-MedoidsSC, we compare two different methods of generating the starting cluster points: Buckshot and Farthest Neighbour Technique.

\begin{table}[]
\centering
\scalebox{0.67}{
\begin{tabular}{@{}ccccccc@{}}
\toprule
Algorithm          & \multicolumn{3}{c}{Farthest Neighbour Technique}     & \multicolumn{3}{c}{Buckshot Technique}                          \\ 
                   & Iris            & Wine            & Seeds            & Iris            & Wine             & Seeds            \\ \midrule
AHC-FPA*           & (50,29,71)      & (10,32,136)     & (8,42,160)       & (50,29,71)      & (10,32,136)      & (8,42,160)       \\
K-Medoids          & (50,45,55)      & (60,41,77)      & (116,61,33)       & (50,45,55)      & (88,74,16)       & (116,61,33)      \\
CSCLP              & (50,50,50)      & (59,71,48)      & (70,70,70)       & (50,50,50)      & (59,71,48)       & (70,70,70)       \\
K-MedoidsSC        & (50,50,50)      & (59,71,48)      & (70,70,70)       & (50,50,50)      & (59,71,48)       & (70,70,70)       \\ 
Real Cluster Size  & (50,50,50)      & (59,71,48)      & (70,70,70)       & (50,50,50)      & (59,71,48)       & (70,70,70)       \\
Initial Points IDs & {[}23,75,119{]} & {[}15,19,118{]} & {[}23,189,204{]} & {[}39,98,113{]} & {[}80,109,135{]} & {[}48,151,152{]} \\ \midrule
\multicolumn{7}{l}{* Does not use any technique to select initial points}                                                         \\ \bottomrule
\end{tabular}}
\caption{Resulting cluster sizes in datasets: Iris, Wine and Seeds, with algorithms: AHC-FPA, K-Medoids, CSCLP and K-MedoidsSC.}
\label{tb:tab1}
\end{table}

As expected, the results demonstrate that both, AHC-FPA and K-Medoids algorithms, fail to meet the expected value in cluster size since they do not use these restrictions. While the two new heuristic proposals meet the size constraints imposed by the user, matching perfectly with the real sizes of the clusters.

Since clustering algorithms define groups that are not known a priori, irrespective of the clustering methods, the final partition of data requires some kind of evaluation \cite{Halkidi:2001}. For this reason, and once the feasibility of the two new proposals in terms of compliance with cluster sizes has been demonstrated, it is important to validate clustering performance. Table \ref{tb:tab2} includes results of four validation measures: Adjusted Rand Index (ARI), Normalised Mutual Information (NMI), Adjusted Mutual Information (AMI), and Silhouette Coefficient $S(i)$.

\begin{table*}[]
\centering
\scalebox{0.67}{
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{{Datasets}} & \multirow{2}{*}{{Algorithm}} & \multicolumn{4}{c}{{Farthest Neighbour Technique}}    & \multicolumn{4}{c}{{Buckshot Algorithm}}             \\ 
                                   &                                     & {ARI} & {AMI} & {NMI} & $\mathbold S (\mathbold i)$ & {ARI} & {AMI} & {NMI} & $\mathbold S (\mathbold i)$ \\ \midrule
\multirow{4}{*}{{Iris}}     & {AHC-FPA*}                   & 0.674        & 0.735        & 0.760        & 0.659           & 0.674        & 0.735        & 0.760        & 0.659           \\  
                                   & {K-Medoids}                  & 0.904        & 0.897        & 0.900        & 0.737           & 0.904        & 0.897        & 0.900        & 0.737           \\ 
                                   & {CSCLP}                      & 0.886        & 0.861        & 0.862        & 0.721           & 0.886        & 0.861        & 0.862        & 0.733           \\ 
                                   & {K-MedoidsSC}                & 0.818        & 0.800        & 0.803        & 0.717           & 0.886        & 0.861        & 0.862        & 0.734           \\ \midrule
\multirow{4}{*}{{Wine}}     & {AHC-FPA*}                   & 0.059        & 0.137        & 0.186        & 0.728           & 0.059        & 0.137        & 0.186        & 0.728           \\ 
                                   & {K-Medoids}                  & 0.347        & 0.363        & 0.373        & 0.758           & 0.208        & 0.196        & 0.221        & 0.757           \\  
                                   & {CSCLP}                      & 0.236        & 0.239        & 0.247        & 0.655           & 0.331        & 0.371        & 0.378        & 0.669           \\ 
                                   & {K-MedoidsSC}                & 0.302        & 0.297        & 0.304        & 0.716           & 0.347        & 0.374        & 0.380        & 0.699           \\ \midrule
\multirow{4}{*}{{Seeds}}    & {AHC-FPA*}                   & 0.223        & 0.286        & 0.379        & 0.659           & 0.223        & 0.286        & 0.379        & 0.659           \\ 
                                   & {K-Medoids}                  & 0.264        & 0.305        & 0.330        & 0.606           & 0.264        & 0.305        & 0.330        & 0.606           \\ 
                                   & {CSCLP}                      & 0.233        & 0.268        & 0.275        & 0.420           & 0.231        & 0.249        & 0.256        & 0.456           \\ 
                                   & {K-MedoidsSC}                & 0.149        & 0.179        & 0.186        & 0.348           & 0.162        & 0.189        & 0.196        & 0.276           \\ \midrule
\multicolumn{10}{l}{* Does not use any technique to select initial points}                                                                                                                           \\ \bottomrule
\end{tabular}}
\caption{Clustering validation results in datasets: Iris, Wine and Seeds, with algorithms: AHC-FPA, K-Medoids, CSCLP and K-MedoidsSC.}
\label{tb:tab2}
\end{table*}

External validation indices (ARI, AMI and NMI), compare properties of an algorithm's proposed clusters against that of known true clusters or ``ground truth" \cite{dalton2009clustering}. If indices values are close to 1, it means that results of clustering are more closely to the ``ground truth'. On the other hand, silhouette coefficient $S(i)$, is an internal validation index, it can be displayed in a graphical way, called silhouette diagram, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity \cite{rousseeuw1987silhouettes}. The index's domain is $-1\leq S(i) \leq 1$, therefore, a higher value of silhouette shows a better clustering performance.

The left side of Figure \ref{fig:validation3}, shows the silhouette diagram of Iris dataset, and the right side indicates the dataset configuration in $\mathbb{R}^{2}$.

\begin{figure}[htb]
\centering 
\includegraphics[width=0.7\textwidth]{./figuras/validation3}
\caption{Clustering in Iris dataset with CSCLP algorithm (Initial points: Buckshot algorithm).} 
\label{fig:validation3}
\end{figure}

The results are notably favourable in Iris dataset, where distribution of the groups is heterogeneous and where we can clearly distinguish two groups \cite{Chavent:98} (see right plot of Figure\ref{fig:validation3}). But also the results in the other two datasets, Wine and Seeds, are positive according to the internal and external validation indices. It demonstrates that the algorithmic proposals presented in this paper are valid.

%------------------------------------------------------------------------------

\subsection{Experimentation in Conference Datasets}
\label{sect:conferences}

After proving that the proposed methods are able to form clusters satisfying the required size restrictions with small datasets, in this part we evaluate our methods to create conference schedules based on the similarity of the papers. Recently, a similar approach has been described by \cite{kvorc06}. In this case the authors also use information from reviews to build the groups, and this information it is not always available.

In generic form, document clustering should be conceived as the partitioning of a documents collection into several groups according to their content \cite{Hu:2008}. Document clustering has been applied to many fields of study, such as: information retrieval, topic detection and content tracking, all of them are intrinsically related to language \cite{Douglass:92}. For the systematic treatment of language, in this paper, natural language processing (NLP) techniques are used to characterise the documents that are intended to be grouped.

A scientific article is a research paper published in specialised journals and conferences. Conferences are usually formed of various sessions where the authors present their selected papers. These sessions are usually thematic and are arranged by the conference programs chair. One of the main drawbacks that arise when organising the sessions of a conference is the large number of topics addressed by the documents presented. In addition, the problem becomes more complex if the number of papers scheduled for each session in a conference is fixed. This  common scenario can be categorised as a problem of document clustering with size constraints.

In data pre-processing, NLP techniques and information retrieval models were applied to obtain a dissimilarity matrix that serves as input for the two new proposed clustering algorithms. A scientific paper is usually composed of several sections that vary according to the style of the journal, the topic area, the style of the author, etc. Machine learning articles, usually follow the IMRaD format (Introduction, Method, Results and Discussion), and there are three sections that have been adopted as a de facto standard: Titles, Keywords and Abstracts. According to \cite{James:2012}, these three elements usually contribute almost 90\% of the related information to the document subject and for this reason they have been used as a data source (in form of dissimilarity matrix), which characterise the differences/similarities between documents. We used a classical scheme for data pre-processing in documents: tokenization, stopwords removal and stemming \cite{Jha:2015}.

Keywords are words or short-phrases (lexemes) that allow classification and orientation of indexing and retrieval systems in databases. In most scientific journals the number of keywords ranges from 3 to 10 and are usually obtained from topics-specific thesaurus \cite{Salim:2011}. On the other hand, paper's titles usually have an average of 8 to 10 words \cite{James:2012}. 
To structure the dissimilarity matrix of titles and keywords, the Jaccard coefficient has been used, since these two elements usually have a smaller number of tokens. The paper's abstract is the most consulted and read section in a scientific article \cite{Salim:2011}. This paper section usually has no more than 250 words as extension. To quantify the similarity/difference (find the dissimilarity matrix) between document abstracts, a vector model has been used with a cosine similarity index on TF-IDF weighting matrix. Thus, we have three dissimilarity matrix corresponding to titles $DM_{t}$, keywords $DM_{k}$ and abstracts $DM_{a}$, and through the following equation, the criteria have been integrated to obtain a total dissimilarity matrix $DM_{T}$:
$$DM_{T}=\sum_{i=1}^{3}(\theta_{i})(DM_{(t,k,a)}); \sum_{i=1}^{3} \theta_{i}=1$$

Where $\theta_{i}$ is an ``importance coefficient" which can be adjusted for the three dissimilarities matrix, some tests with different values have been performed to refine the total matrix. The corpus of abstracts in a paper usually provide a greater amount of useful information than the other two elements (title and keywords) \cite{Salim:2011}, so the weighting of this coefficient in our work will be greater. After several tests, we notice that the best configuration is the one that combines the following coefficient $\theta_{i}$ values: $\theta_{1}=0.55$, $\theta_{2}=0.35$ and $\theta_{1}=0.10$, for abstracts, keywords and titles, respectively. For this reason we will use this configuration to evaluate the performance of our algorithms. % The coefficient that we have used for the experimental tests is $\theta_{1}=0.55$, $\theta_{2}=0.35$ and $\theta_{1}=0.10$

To test the operation of the two new algorithms, in document clustering, three datasets of scientific conferences have been used. The first dataset corresponds to the ``13th International Conference on Machine Learning and Applications, ICMLA-14". For the first dataset construction we used data scraping techniques from its website \cite{ICMLA:2014}. The other two datasets were obtained from the UCI Repository \cite{UCI:1998} corresponding to the ``27th Conference on Artificial Intelligence of Association for the Advancement of Artificial Intelligence, AAAI-13" and the ``28th Conference on Artificial Intelligence of Association for the Advancement of Artificial Intelligence, AAAI-14". The ICMLA-14 conference dataset consists of 69 papers, while AAAI-2013 and AAAI-2014 have 150 and 398 documents, respectively. It is important to clarify that to define the external validation indices we consider that the ground truth of classes assignment, provided by the conference schedule, is the ``ideal" or ``real" solution. For all the conferences, we assume that the papers have been grouped manually into sessions by the program chairs considering the similarity among papers. We examined different scenarios by combining unequal cluster sizes $E_{j}$, with different values in the number of clusters $k$. The performance results obtained by applying the new algorithms, CSCLP and K-MedoidsSC, over the three datasets are summarised in Table \ref{tb:tab4}. Note that we do not include other semi-supervised clustering techniques in the experiments since they need a Euclidean metric defined in the elements for grouping them and here we do not have this property. The exception is the work presented in \cite{kvorc06}, but this method requires the reviews and they are not available in the studied datasets. 


%The datasets AAAI-13 and AAAI-14 do not have as attributes the conference sessions organisation by theme, then, the cluster's size were unknown. We use once again web scrapping on their  official websites for extracting the final organisation of thematic sessions.

%Since equation $DM_{T}$ describes the total dissimilarity matrix, where $\theta_{i}$ is an ``importance coefficient" which can be adjusted for the three dissimilarities matrix, some tests with different values have been performed to refine the total matrix. 
 
%Table \ref{tb:tab3} describes three possible scenarios contemplated for experimentation.
%
%\begin{table}[]
%\centering
%\scalebox{0.70}{
%\begin{tabular}{|c|c|c|c|l}
%\cline{1-4}
%                             & $\mathbold \theta_{\mathbold 1}$ & $\mathbold \theta_{\mathbold 2}$ & $\mathbold \theta_{\mathbold 3}$ &  \\ \cline{1-4}
%\textbf{Scenario I (Sc-1)}   & 0.10                  & 0.35                  & 0.55                  &  \\ \cline{1-4}
%\textbf{Scenario II (Sc-2)}  & 0.15                  & 0.25                  & 0.60                  &  \\ \cline{1-4}
%\textbf{Scenario III (Sc-3)} & 0.05                  & 0.20                  & 0.75                  &  \\ \cline{1-4}
%\end{tabular}}
%\caption{$\theta_{i}$ values in total dissimilarity matrix.}
%\label{tb:tab3}
%\end{table}

%The ICMLA-14 conference dataset consists of 69 papers. It is important to clarify that to define the external validation indices we consider that the ground truth of classes assignment, provided by the conference schedule, is the ``ideal" or ``real" solution. For all the conferences, we assume that the papers have been grouped manually into sessions by the program chairs considering the similarity among papers. 
%In the program schedule of ICMLA-14, the conference organisers  established a total of 14 sessions ($k=14$), but this number can be reduced to 11 ($k=11$), because some of the sessions have the same topic (e.g. Neural Networks I and II, documents 20 to 29). Therefore, in our tests, we also consider two $k$ values: 11 and 14, respectively. In the conference webs can be consulted the total program schedule of the three conferences (ICMLA-2014, AAAI-2013 and AAAI-2014) organised by sessions.



%\begin{table}[]
%\centering
%\scalebox{0.68}{
%\begin{tabular}{|l|c|c|c|c|}
%\hline
%\multicolumn{1}{|c|}{\textbf{Algorithm}}   & \textbf{ARI} & \textbf{AMI} & \textbf{NMI} & $\mathbold S (\mathbold i)$ \\ \hline
%\multicolumn{1}{|c|}{\textbf{CSCLP}}       & 0.0946       & 0.1457       & 0.5328       & -0.2206         \\ \hline
%\multicolumn{1}{|c|}{\textbf{K-MedoidsSC}} & 0.0347       & 0.0662       & 0.4358       & -0.2396         \\ \hline
%\multicolumn{5}{|l|}{Number of Clusters ($k$): 14}                                                        \\ \hline
%\multicolumn{5}{|l|}{Initial Points IDs: {[}25,27,48,5,4,12,9,67,42,30,33,40,2,15{]}}        \\ \hline
%\multicolumn{5}{|l|}{Cluster Sizes ($E_j$): (4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)}                   \\ \hline
%\multicolumn{1}{|c|}{\textbf{Algorithm}}   & \textbf{ARI} & \textbf{AMI} & \textbf{NMI} & $\mathbold S (\mathbold i)$ \\ \hline
%\multicolumn{1}{|c|}{\textbf{CSCLP}}       & 0.0527       & 0.0845       & 0.3919       & -0.2511         \\ \hline
%\multicolumn{1}{|c|}{\textbf{K-MedoidsSC}} & 0.0465       & 0.0607       & 0.3384       & -0.2317         \\ \hline
%\multicolumn{5}{|l|}{Number of Clusters ($k$): 11}                                                        \\ \hline
%\multicolumn{5}{|l|}{Initial Points IDs: {[}25,27,48,5,12,9,30,33,40,2,15{]}}                             \\ \hline
%\multicolumn{5}{|l|}{Cluster Sizes ($E_j$): (5, 5, 5, 5, 5, 5, 5, 5, 9, 10, 10)}                          \\ \hline
%\end{tabular}}
%\caption{Clustering results in ICMLA-14 dataset, $k=14$ and $k=11$, Sc-1.
%\label{tb:tab4}}
%\end{table}

\begin{table*}[]
\centering
\scalebox{0.54}{
\begin{tabular}{c|c|cccccc|cccccc}
\toprule
\multirow{2}{*}{{Datasets}} & \multirow{2}{*}{{Algorithm}} & \multicolumn{6}{c|}{{Farthest Neighbour Technique}}    & \multicolumn{6}{c}{{Buckshot Algorithm}}             \\ 
                                   &                                     & {${k}$} & {Cluster Size $c_{j}$} & ARI & AMI & NMI & $S(i)$ & {${k}$} & {Cluster Size $c_{j}$} & ARI & AMI & NMI & $S(i)$  \\ \midrule
\multirow{2}{*}{{AAAI-13}}        & {CSCLP}                      & 3   & (45,52,53)  & 0.029 & 0.030  & 0.042 & 0.028            & 3   & (45,52,53)  & 0.036 & 0.037  & 0.049 & 0.031                    \\ 
                                  & {K-MedoidsSC}                & 3   & (45,52,53)  & 0.010 & 0.012  & 0.024 & 0.023            & 3   & (45,52,53)  & 0.018 & 0.016  & 0.028 & 0.030                    \\ \midrule
\multirow{2}{*}{{AAAI-14}}         & {CSCLP}                      & 11  & (10,11,18,19,21,25,30,42,45,57,120)  & 0.079 & 0.128  & 0.185 & 0.040            
& 11  & (10,11,18,19,21,25,30,42,45,57,120)  & 0.074 & 0.120  & 0.178 & 0.037                \\ 
                                  & {K-MedoidsSC}                & 11  & (10,11,18,19,21,25,30,42,45,57,120)  & 0.025 & 0.074  & 0.135 & 0.035            
& 11  & (10,11,18,19,21,25,30,42,45,57,120)  & 0.048 & 0.085  & 0.145 & 0.036                          
\\ \midrule
\multirow{2}{*}{{ICMLA-14}}        & {CSCLP}                      & 14   & (4,5,5,5,5,5,5,5,5,5,5,5,5,5)  & 0.040 & 0.074  & 0.494 & 0.229            
& 14   & (4,5,5,5,5,5,5,5,5,5,5,5,5,5)  & 0.094 & 0.146  & 0.533 & 0.234          \\ 
                                   & {K-MedoidsSC}                & 14   & (4,5,5,5,5,5,5,5,5,5,5,5,5,5)  & 0.063 & 0.107  & 0.512 & 0.230            
& 14   & (4,5,5,5,5,5,5,5,5,5,5,5,5,5)  & 0.048 & 0.079  & 0.497 & 0.217           \\ \midrule
\end{tabular}}
\caption{Clustering results in datasets: AAAI-13, AAAI-14 and ICMLA-14 with algorithms: CSCLP and K-MedoidsSC and two initial points methods (Farthest Neighbour and Buckshot).}
\label{tb:tab4}
\end{table*}

The $S(i)$ values, for both algorithms, shown in Table \ref{tb:tab4} are appropriate, considering that the $k$ value is high compared with the dataset size $n$. If we analyse the external validation indices, we see that inisialisation by Buckshot Algorithmt obtains better performance, in general, than Farthest Neighbour Technique. When comparing methods, CSCLP presents better results than K-MedoidsSC.
%Although CSCLP is able to build clusters that follow completely the size restrictions in all cases, it is not the case for K-MedoidsSC algorithm. In one case K-MedoidsSC gets values of cluster sizes different from the desired cluster sizes (see Buckshot scenario in Table \ref{tb:tab4}). Probably, these malfunctions are caused because K-Means algorithm, as well as its variations K-Medoids, K-Modes, etc., does not work properly  when the datasets are especially homogeneous (elements very similar) and when the value of $n$ is high and the $k$ value is low.% On the other hand, CSCLP algorithm has perfectly complied with the clusters sizes imposed by the user. 

%The $S(i)$ values, for both algorithms, shown in Table \ref{tb:tab4} are very low, which implies that the resulting clusters in some cases are especially heterogeneous and in most cases clusters have been formed artificially. This is mainly because the $k$ value is too high compared with the dataset size $n$ \cite{rousseeuw1987silhouettes}.




%After clustering process in ICMLA-14, both new algorithms, have detected and clustered the paper's pattern with theme ``Neural Networks", it can be seen in the right side of Figure \ref{fig:experimen3}, and the left side shows the silhouette diagram of each cluster. 
%
%\begin{figure}[htb]
%\centering 
%\includegraphics[width=0.58\textwidth]{./figuras/silueta1}
%\caption{Clustering in ICMLA-14 dataset with CSCLP, $k=11$, Sc-1, Initial Points: Buckshot.} 
%\label{fig:experimen3}
%\end{figure}



%\begin{table}[]
%\centering
%\scalebox{0.70}{
%\begin{tabular}{|c|c|c|}
%\hline
%\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Number of \\ Clusters ($\mathbold k$)\end{tabular}}} & \multicolumn{2}{c|}{\textbf{Cluster Sizes}}                                                                                                                                     \\ \cline{2-3} 
%                                                                                              & \textbf{Desired ($\mathbold E_{\mathbold j}$)}                                                            & \textbf{Obtained ($\mathbold c_{\mathbold j}$)}                                                             \\ \hline
%14                                                                                            & \begin{tabular}[c]{@{}c@{}}(4, 5, 5, 5, 5, 5, 5, \\ 5, 5, 5, 5, 5, 5, 5)\end{tabular} & \begin{tabular}[c]{@{}c@{}}(4, 5, 10, 2, 0, 0, 6, \\ 5, 5, 5, 12, 5, 5, 5)\end{tabular} \\ \hline
%11                                                                                            & \begin{tabular}[c]{@{}c@{}}(5, 5, 5, 5, 5, 5, \\ 5, 5, 9, 10, 10)\end{tabular}        & \begin{tabular}[c]{@{}c@{}}(5, 0, 5, 0, 11, 6,\\  5, 5, 14, 9, 9)\end{tabular}          \\ \hline
%\end{tabular}}
%\caption{Divergence in cluster sizes for K-MedoidsSC algorithm, ICMLA-14 dataset, Sc-1.}
%\label{tb:tab5}
%\end{table}

%In AAAI-14 dataset, as in ICMLA-14 dataset, the K-MedoidsSC algorithm has not met the desired cluster sizes.As an example of situations where K-MedoidsSC meets established size constraints, we consider the following scenario. We set the size of the objective clusters by using the papers presented at the conference distributed by day (three in this case). In this scenario the two algorithms fulfil with the established values as size constraints for the three clusters. 

%The results are shown in Table \ref{tb:tab6}. We can see that there is a small improvement in the $S(i)$ values in both algorithms when $k$ decreases from 14 or 11 to 3.

%\begin{table}[]
%\centering
%\scalebox{0.70}{
%\begin{tabular}{|l|c|c|c|c|}
%\hline
%\multicolumn{1}{|c|}{\textbf{Algorithm}}   & \textbf{ARI} & \textbf{AMI} & \textbf{NMI} & $\mathbold S (\mathbold i)$ \\ \hline
%\multicolumn{1}{|c|}{\textbf{CSCLP}}       & 0.0399       & 0.0441       & 0.0724       & -0.0094         \\ \hline
%\multicolumn{1}{|c|}{\textbf{K-MedoidsSC}} & 0.0969       & 0.0542       & 0.0821       & -0.0371         \\ \hline
%\multicolumn{5}{|l|}{Number of Clusters ($k$): 3}                                                         \\ \hline
%\multicolumn{5}{|l|}{Initial Points IDs: {[}10, 25, 33{]}}                                                \\ \hline
%\multicolumn{5}{|l|}{Cluster Sizes ($E_j$):(15, 20, 34)}                                                  \\ \hline
%\end{tabular}}
%\caption{Clustering results in ICMLA-14 dataset, $k=3$, Sc-1.}
%\label{tb:tab6}
%\end{table}
%
%When $k=3$, the mean value of silhouette coefficient in CSCLP and K-MedoidsSC is -0.02325, which a priori implies that the groups have been created in a forced or artificial way, but in comparison with -0.04113 that is the ``real" value of the silhouette coefficient (where the value of $k$ number of groups for each session is set according to the organiser's criteria of the conference), our approach presents a better performance.
%
%Extrapolating the above ICMLA-14 dataset procedure, similar tests have been run with datasets AAAI-13 (149 documents) and AAAI-14 (398 documents). The summary of the main performance results achieved is presented in Table \ref{tb:tab7}. 
%
%\begin{table}[]
%\centering
%\scalebox{0.68}{
%\begin{tabular}{|l|c|c|c|c|}
%\hline
%\multicolumn{1}{|c|}{\textbf{Algorithm}}                 & \textbf{ARI}               & \textbf{AMI}              & \textbf{NMI}              & $\mathbold S (\mathbold i)$              \\ \hline
%\multicolumn{1}{|c|}{\textbf{CSCLP}}                     & 0.0080                     & 0.0071                    & 0.0195                    & -0.0336                      \\ \hline
%\multicolumn{1}{|c|}{\textbf{K-MedoidsSC}}               & 0.0126                     & 0.0115                    & 0.0239                    & -0.0306                      \\ \hline
%\multicolumn{5}{|l|}{Number of Clusters ($k$): 3}                                                                                                                            \\ \hline
%\multicolumn{5}{|l|}{Initial Points IDs: {[}42, 96, 118{]}}                                                                                                                  \\ \hline
%\multicolumn{5}{|l|}{Cluster Sizes ($E_j$): (44, 52, 53)}                                                                                                                    \\ \hline
%\multicolumn{1}{|c|}{\textbf{Algorithm}}                 & \textbf{ARI}               & \textbf{AMI}              & \textbf{NMI}              & $\mathbold S (\mathbold i)$              \\ \hline
%\multicolumn{1}{|c|}{\textbf{CSCLP}}                     & 0.0487                     & 0.1204                    & 0.1782                    & -0.2341                      \\ \hline
%\multicolumn{1}{|c|}{\textbf{K-MedoidsSC}}               & 0.0138                     & 0.0312                    & 0.0894                    & -0.1458                      \\ \hline
%\multicolumn{5}{|l|}{Number of Clusters ($k$): 11}                                                                                                                           \\ \hline
%\multicolumn{5}{|l|}{\begin{tabular}[c]{@{}l@{}}Initial Points IDs: {[}4, 113, 145, 190, 195, 222, \\                               256, 266, 268, 300, 304{]}\end{tabular}} \\ \hline
%\multicolumn{5}{|l|}{\begin{tabular}[c]{@{}l@{}}Cluster Sizes ($E_j$): (10, 11, 18, 19, 21, 25, \\                                       30, 42, 45, 57, 120)\end{tabular}}  \\ \hline
%\end{tabular}}
%\caption{Clustering results in AAAI-13 and AAAI-14 datasets, Sc-1.}
%\label{tb:tab7}
%\end{table}


We also have developed a web system, called ADoCS, implementing the CSCLP algorithm that could help programs chairs to organise conference program schedules. This tool could be also useful to other related tasks in clustering documents with restrictions, such as find groups of papers to be assigned to reviews. The web tool can be used free of charge in the url: \url{https://ceferra.shinyapps.io/ADoCS/}. 
ADoCS tool has been implemented totally with R \cite{Rproy}.
%This system is a free software language for statistical computing and graphics. There is a plethora of different libraries that makes R a powerful environment for developing software related to data analysis in general, and computational linguistics in particular.
The graphical user interface has been developed by means of the Shiny package \cite{shiny}.
%This package constitutes a framework for building  graphical  applications in R. Shiny is a powerful package for converting basic R scripts into interactive web applications without requiring programming web knowledge.
As a result we obtain an interactive web application that can be executed locally using a web browser, or can be uploaded to a shiny application server where the tool is available for a general use. In this case, we have uploaded the application to the free server. % \url{http://www.shinyapps.io/}. 
The ADoCS source code and some datasets about conferences to test the tool can be found in \url{https://github.com/dievalhu/ADoCS}. 

%------------------------------------------------------------------------------

\section{Conclusions}
\label{sect:conclusions}

In this paper we have presented two novel algorithms for semi-supervised clustering that allow constraint the sizes of the clusters. The first one, CSCLP algorithm, is based on optimisation techniques, while the second, K-MedoidsSC algorithm, represents a variation of the original K-Medoids algorithm for considering size constrains in the clusters.
Empirical evidence in benchmarking datasets and conference datasets has shown that the proposed algorithms can solve clustering problems with size constraints. We have shown the application of these methods on the automatic arranging of papers to create an appropriate conference schedule which sessions. 
%However, K-MedoidsSC algorithm is not always able to find a correct grouping in cases with the clusters to discover are relatively small with respect to the number of elements.
%Arranging papers to create an appropriate conference schedule which sessions contain papers with common topics is a hard task, specially when the number of papers is high. Machine learning  offers techniques that can automatise this task by the use of NLP for extracting features. In this context, organising a  conference schedule can be seen as a semi-supervised clustering. 
As future work, we are interested in developing conceptual clustering methods to extract topics from the created clusters.
%We also plan to investigate techniques to improve the correction phase of the K-MedoidsSC algorithm in such a way, that the algorithm could be able to get a correct solution in all the cases.

%------------------------------------------------------------------------------


%------------------------------------------------------------------------------
%\subsection{Required Packages}
%
%\begin{itemize}
%\item
%\texttt{url} \cite{url-package} (included also by \texttt{hyperref} automatically) -- to provide URL rendering support for the
%monospaced font, which takes care of special characters as well as line 
%wrapping.
%
%\item
%\texttt{hyperref} \cite{hyperref-package} -- to allow hyperlinking of URLs and
%cross references within an article.
%Its options are set to either \verb+letterpaper+ or \verb+a4paper+, depending
%on the \verb+\documentclass+ options.
%
%\item
%\texttt{graphicx} \cite{graphicx-package} -- the standard package for rendering
%PNG, JPG, and PDF graphic images, primarily in \texttt{figure} environments.
%
%\item
%optional \texttt{mathptmx} \cite{mathptmx-package} -- Times base font for compactness
%(use with the \texttt{withtimes} {\easychair} option).
%
%\item
%\texttt{helvet} \cite{helvet-package} -- Helvetica as {\sf sans-serif}.
%
%\item
%\texttt{listings} \cite{listings-package} -- to allow highlighted source code 
%listing styles.
%
%\item
%\texttt{latexsym} \cite{latexsym-package} -- to provide common math and other 
%symbols.
%
%\item
%\texttt{amsthm} \cite{amsthm-package} -- to provide {\AmS} theorem-like 
%environments.
%
%\item
%\texttt{empheq} \cite{empheq-package} -- to provide equation environments, etc.
%
%\item
%\texttt{geometry} \cite{geometry-package} -- to set {\easychair} margins, 
%outlined in Section~\ref{sect:generalities}.
%
%\item
%\texttt{lastpage} \cite{lastpage-package} -- to allow computationally
%referencing the last page.
%
%\item
%\texttt{fancyhdr} \cite{fancyhdr-package} -- for running heads.
%
%\item
%\texttt{footmisc} \cite{footmisc-package} -- to ensure that footnotes are
%always at the bottom.
%
%\item
%optional \texttt{makeidx} \cite{makeidx-package} -- for index generation
%(use with the \texttt{thesis} {\easychair} option).
%
%\item
%\texttt{eso-pic} \cite{eso-pic-package} -- for draft versions and checking page
%overlflows vs. a border drawn around the headers, footers, and the main body of
%the article.
%
%\item
%\texttt{rotating} \cite{rotating-package} -- to rotate floats (figures and
%tables) on the page, when wide tables or figures do not fit in portrait layout.
%
%\item
%\texttt{pdflscape} \cite{pdflscape-package} -- similar to \texttt{rotating}, 
%but also allows rotating text to make it conveniently viewable in a PDF 
%viewer that supports individual rotated pages.
%A possible disadvantage is that a page break is forced, which may create
%gaps before or after the landscape page.
%
%\item
%\texttt{algorithm2e} \cite{algorithm2e-package} -- provides a figure-like
%algorithm environment for formal algorithm presentation with highlighting.
%
%\end{itemize}
%
%\subsection{Tables}
%
%Many page overflows happen because of large tables. In many case these
%overflows can be easily removed by slightly reducing padding added by
%\LaTeX\ to every column. It is controlled by the \LaTeX\ command
%\verb|\tabcolsep| whose value by default is 6pt. Even small changes in
%the value of this command may give drastic reductions in the width of
%tables. This is illustrated in Figure~\ref{fig:tabcolsep} on
%page~\pageref{fig:tabcolsep}. Note though that there is no free lunch:
%smaller values for this command may result in lower readability.
%
%%------------------------------------------------------------
%\begin{figure}[tb]\small
%\begin{center}
%  \begin{tabular}{lrrrrrrrr}
%    \hline
%    ATP System            & LTB & Avg  &Prfs & SOTA & \multicolumn{1}{c}{$\mu$} & CYC & MZR & SMO \\
%   \hline
%    Vampire-LTB 11.0      &  69 & 24.5 &  69 & 0.37 & 28.1 &  23 &  22 &  24 \\
%    iProver-SInE 0.7      &  67 & 76.5 &   0 & 0.36 &  8.8 &  28 &  14 &  25 \\
%   \hline
%  \end{tabular}
%\end{center}
%
%\begin{center}\renewcommand{\tabcolsep}{5pt}
%  \begin{tabular}{lrrrrrrrr}
%    \hline
%    ATP System            & LTB & Avg  &Prfs & SOTA & \multicolumn{1}{c}{$\mu$} & CYC & MZR & SMO \\
%   \hline
%    Vampire-LTB 11.0      &  69 & 24.5 &  69 & 0.37 & 28.1 &  23 &  22 &  24 \\
%    iProver-SInE 0.7      &  67 & 76.5 &   0 & 0.36 &  8.8 &  28 &  14 &  25 \\
%   \hline
%  \end{tabular}
%\end{center}
%
%\begin{center}\renewcommand{\tabcolsep}{3pt}
%  \begin{tabular}{lrrrrrrrr}
%    \hline
%    ATP System            & LTB & Avg  &Prfs & SOTA & \multicolumn{1}{c}{$\mu$} & CYC & MZR & SMO \\
%   \hline
%    Vampire-LTB 11.0      &  69 & 24.5 &  69 & 0.37 & 28.1 &  23 &  22 &  24 \\
%    iProver-SInE 0.7      &  67 & 76.5 &   0 & 0.36 &  8.8 &  28 &  14 &  25 \\
%   \hline
%  \end{tabular}
%\end{center}
%
%\begin{center}\renewcommand{\tabcolsep}{1pt}
%  \begin{tabular}{lrrrrrrrr}
%    \hline
%    ATP System            & LTB & Avg  &Prfs & SOTA & \multicolumn{1}{c}{$\mu$} & CYC & MZR & SMO \\
%   \hline
%    Vampire-LTB 11.0      &  69 & 24.5 &  69 & 0.37 & 28.1 &  23 &  22 &  24 \\
%    iProver-SInE 0.7      &  67 & 76.5 &   0 & 0.36 &  8.8 &  28 &  14 &  25 \\
%   \hline
%  \end{tabular}
%\end{center}
%\normalsize
%
%\caption{Original table and tables with \texttt{tabcolsep} set to 5pt,
%  3pt, and 1pt
%  \label{fig:tabcolsep}}
%
%\end{figure}

%------------------------------------------------------------------------------
% Refs:
\label{sect:bib}
\bibliographystyle{plain}
%\bibliographystyle{alpha}
%\bibliographystyle{unsrt}
%\bibliographystyle{abbrv}
\bibliography{easychair}


%------------------------------------------------------------------------------
% Index
%\printindex

%------------------------------------------------------------------------------
\end{document}

% EOF
